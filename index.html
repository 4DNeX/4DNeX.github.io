<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Feed-Forward 4D Generative Modeling Made Easy</title>
  <link href="./files/style.css" rel="stylesheet">
  <script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script>
  <script type="text/javascript" src="./files/jquery.js"></script>
  <style>
    .divider {
      border-right: 2px dashed #737373;
      width: 2px;
    }
  </style>
  <style>
    .divider_horizontal {
      border-top: 2px dashed #737373;
      display: block;
      width: 100%;
      margin: 10px 0;
    }
  </style>
  <!-- Begin new styles from demos page -->
  <style>
    body {
      padding: 2em;
      font-family: sans-serif;
    }

    iframe {
      border-radius: 0.5em;
      width: 27.5em;
      height: 27.5em;
      border: none;
      box-shadow: 0 0 1em 0em rgba(0, 0, 0, 0.15);
    }

    @media (max-width: 768px) {
      iframe {
        width: 100%;
        height: 30em;
      }
    }

    a,
    a:link {
      color: #777;
    }

    /* Styles for the instruction icons and text */
    .instructions {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      gap: 1.2em;
      text-align: center;
      padding: 0.6em;
    }

    .instruction-item {
      display: flex;
      align-items: center;
      gap: 0.5em;
      font-size: 1.2em;
    }

    .instruction-item img {
      width: 36px;
      height: 36px;
    }

    .instructions_small {
      display: flex;
      justify-content: center;
      align-items: center;
      flex-wrap: wrap;
      gap: 1em;
      text-align: center;
      padding: 0.5em;
    }

    .instruction-item_small {
      display: flex;
      align-items: center;
      gap: 0.3em;
      font-size: 1.0em;
    }

    .instruction-item_small img {
      width: 30px;
      height: 30px;
    }
  </style>
  <!-- End new styles -->
</head>

<body>
  <div class="content">
    <h1 style="line-height: 0.75;">
      <strong>Feed-Forward 4D Generative Modeling Made Easy</strong>
    </h1>
    <!-- <p id="authors">
      <span>
        <a href="https://junyi42.github.io/">Junyi Zhang<sup>1</sup></a>
      </span>
      <span>
        <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ">Charles Herrmann<sup>2,+</sup></a>
      </span>
      <span>
        <a href="https://hurjunhwa.github.io/">Junhwa Hur<sup>2</sup></a>
      </span>
      <span>
        <a href="https://varunjampani.github.io/">Varun Jampani<sup>3</sup></a>
      </span>
      <span>
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell<sup>1</sup></a>
      </span>
      <br>
      <span>
        <a href="https://scholar.google.com/citations?user=xZRRr-IAAAAJ&hl">Forrester Cole<sup>2</sup></a>
      </span>
      <span>
        <a href="https://deqings.github.io/">Deqing Sun<sup>2,*</sup></a>
      </span>
      <span>
        <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang<sup>2,4,*</sup></a>
      </span>
      <br>
      <span class="institution"><a href="https://bair.berkeley.edu/"><sup>1</sup> UC Berkeley</a>
        <a href="https://deepmind.google/"><sup>2</sup> Google DeepMind</a>
        <a href="https://stability.ai/"><sup>3</sup> Stability AI</a>
        <a href="https://www.ucmerced.edu/"><sup>4</sup> UC Merced</a></span>
      <span class="institution"></span>
      <span class="institution">(+: project lead, *: equal contribution)</span>
      <br>
      <span class="conference">ICLR 2025 (Spotlight)</span>
      <br>
      <br>
    </p>
    </p> -->

    <font size="+2">
      <p style="text-align: center;">
        <a href="" target="_blank">[Arxiv]</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="" target="_blank">[YouTube]</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="" target="_blank">[Code]</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="interactive-results/demo1.html" target="_blank">[Interactive Results]</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <!-- <a href="https://x.com/junyi42/status/1770799360353145037" target="_blank">[Twitter Post]</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
        <!-- <a href="https://www.youtube.com/watch?v=fyhg50qlNW8" target="_blank">[Video]</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
        <!-- <a href="files/bibtex.txt" target="_blank">[BibTeX]</a> -->
      </p>
    </font>

    <a style="text-align:center">
      <font size="+1">
        <strong>4DNeX</strong> is a <strong>feed-forward</strong> framework for generating 4D scene representations from a <strong>single image</strong> by fine-tuning a video diffusion model. It produces high-quality <strong>dynamic point clouds</strong> and enables downstream tasks such as <strong>novel-view video synthesis</strong> with strong generalizability.
      </font>
    </a>

    <!-- add video files/teaser_vid_v2_lowres.mp4 -->
    <!-- <video width="100%" controls> -->
    <video width="100%" controls poster="./assets/teaser_poster.png">
      <source src="./assets/teaser.mp4" type="video/mp4">
    </video>
    
    <br>
    <br>

  </div>

  <!-- Begin new content: Demos -->
<div class="content">
    <h2>Interactive 4D Visualization</h2>
    <p>Explore the 4D reconstruction results on various dynamic scenes. For more results, please visit the <a href="interactive-results/demo1.html" style="font-weight: bold; color: rgb(238, 60, 223);">interactive results</a>.</p>
    <!-- Unified Instruction Section -->
    <div class="instructions" style="margin-top: -0.5em; display: flex; flex-direction: column; align-items: center; gap: 1em; text-align: center;">

      <!-- Row 1: Mouse Controls -->
      <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 2em; align-items: center;">
        <div style="display: flex; align-items: center; gap: 0.5em;">
          <img src="icons/left-click.png" alt="Left Click" style="width: 36px; height: 36px;">
          <span>Drag with <em>left</em> click to <strong>rotate</strong> view</span>
        </div>
        <div style="display: flex; align-items: center; gap: 0.5em;">
          <img src="icons/scroll-wheel.png" alt="Scroll Wheel" style="width: 36px; height: 36px;">
          <span>Scroll to <strong>zoom</strong> in/out</span>
        </div>
        <div style="display: flex; align-items: center; gap: 0.5em;">
          <img src="icons/right-click.png" alt="Right Click" style="width: 36px; height: 36px;">
          <span>Drag with <em>right</em> click to <strong>move</strong> view</span>
        </div>
      </div>

      <!-- Row 2: Keyboard Controls -->
      <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 2em; align-items: center;">
        <div style="display: flex; align-items: center; gap: 0.3em;">
          <img src="icons/letter-w.png" alt="W" style="width: 30px; height: 30px;">
          <img src="icons/letter-s.png" alt="S" style="width: 30px; height: 30px;">
          <span>Move forward/backward</span>
        </div>
        <div style="display: flex; align-items: center; gap: 0.3em;">
          <img src="icons/letter-a.png" alt="A" style="width: 30px; height: 30px;">
          <img src="icons/letter-d.png" alt="D" style="width: 30px; height: 30px;">
          <span>Move left/right</span>
        </div>
        <div style="display: flex; align-items: center; gap: 0.3em;">
          <img src="icons/letter-q.png" alt="Q" style="width: 30px; height: 30px;">
          <img src="icons/letter-e.png" alt="E" style="width: 30px; height: 30px;">
          <span>Move up/down</span>
        </div>
      </div>

    </div>

  <!-- Downsampling note -->
  <p style="text-align: center; font-size: 1em; padding: 0em; color: #555;">
  </p>

  <!-- Wrapper for iframes -->
  <div id="wrapper" style="
        display: flex;
        flex-wrap: wrap;
        justify-content: center;
        align-items: center;
        gap: 2em;
      ">
    <!-- Insert two iframes -->
    <!-- Wrapper 1 -->
  <div style="margin-bottom: 2em; text-align: center;">
    <iframe
      id="viser-frame-00000"
      src="https://4DNeX.github.io/build/?playbackPath=https://4DNeX.github.io/recordings/recording_0015_00005.viser&initDistanceScale=0.1&initHeightOffset=4.0"
      width="600"
      height="300"
      frameborder="0"
      style="border-radius: 8px;">
    </iframe>
    <br>
    <button onclick="window.open(document.getElementById('viser-frame-00000').src, '_blank')">
      Open in New Tab for Better Visualization
    </button>
  </div>

  <!-- Wrapper 2 -->
  <div style="margin-bottom: 2em; text-align: center;">
    <iframe
      id="viser-frame-00006"
      src="https://4DNeX.github.io/build/?playbackPath=https://4DNeX.github.io/recordings/recording_0014_00005.viser&initDistanceScale=0.1&initHeightOffset=4.0"
      width="600"
      height="300"
      frameborder="0"
      style="border-radius: 8px;">
    </iframe>
    <br>
    <button onclick="window.open(document.getElementById('viser-frame-00006').src, '_blank')">
      Open in New Tab for Better Visualization
    </button>
  </div>
</div>

  </div>

  <!-- End new content -->

  <div class="content">
    <h2 style="text-align:center;">Abstract</h2>
    <font size="-0.">
      <p>
        We present <strong>4DNeX</strong>, the first feed-forward framework for generating 4D (<i>i.e.</i>, dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model.
        Specifically:
        <strong>1)</strong> To alleviate the scarcity of 4D data, we construct <em>4DNeX-1M</em>, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches.
        <strong>2)</strong> We introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry.
        <strong>3)</strong> We propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for the 4D generation task.
        4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX achieves competitive performance compared to existing 4D generation approaches, offering a scalable and generalizable solution for single-image-based 4D scene generation.
      </p>
    </font>
  <!-- <img class="summary-img" src="./assets/teaser.png" style="width:100%;"> -->
  <video width="100%" controls poster="./assets/overview_poster.png">
    <source src="./assets/overview.mp4" type="video/mp4">
  </video>
  </div>


  <div class="content">
    <h2 style="text-align:center;">Method</h2>
    <font size="-0.">
      <p style="text-align: justify; font-size: 1.05em;">
        Given <stronga single RGB image></strong> and an initialized XYZ map, <strong>4DNeX</strong> encodes both inputs using a VAE encoder and fuses them via <strong>width-wise concatenation</strong>. 
        The fused latent, combined with a noise latent and a guided mask, is processed by a LoRA-tuned Wan-DiT model to jointly generate <strong>RGB and XYZ videos</strong>. 
        A lightweight post-optimization step recovers camera parameters and depth maps from the predicted outputs, yielding consistent <strong>dynamic point clouds</strong>.
      </p>
    </font>
    <img class="summary-img" src="./assets/pipeline.png" style="width:100%;">
  </div>

  <div class="content">
    <h2 style="text-align:center;">Fusion strategies</h2>
    <font size="-0.">
      <p>
        We explore five fusion strategies to combine <strong>RGB</strong> and <strong>XYZ</strong> inputs:
      </p>
      <ul>
        <li><strong>(a) Channel-wise:</strong> Breaks pretrained distribution; often fails to generate meaningful outputs.</li>
        <li><strong>(b) Batch-wise:</strong> Preserves unimodal quality but lacks RGBâ€“XYZ alignment.</li>
        <li><strong>(c) Frame-wise:</strong> Keeps temporal order but weak cross-modal interaction.</li>
        <li><strong>(d) Height-wise:</strong> Slightly better alignment, still suboptimal.</li>
        <li><strong>(e) Width-wise:</strong> Brings RGB and XYZ tokens closer, enabling coherent joint generation.</li>
      </ul>
      <p>
    </font>
    <!-- <img class="summary-img" src="./assets/fusion.png" style="width:100%;"> -->
    <img class="summary-img" src="./assets/analysis.png" style="width:100%;">
    <video width="100%" controls>
      <source src="./assets/ab.mp4" type="video/mp4">
    </video>
  </div>



  <div class="content">
    <h2 style="text-align:center;">Results - 6D (RGB+XYZ) Video</h2>
    <font size="-0.">
      <p style="text-align: justify; font-size: 1.05em;">
        <strong>4DNeX</strong> generates paired <strong>RGB</strong> and <strong>XYZ</strong> sequences, forming a unified <strong>6D representation</strong> of dynamic scenes.
      </p>
    </font>
    <video width="100%" controls>
      <source src="./assets/6D_video.mp4" type="video/mp4">
    </video>
  </div>


  <div class="content">
    <h2 style="text-align:center;">Results - Dynamic Point Cloud</h2>
    <font size="-0.">
      <p style="text-align: justify; font-size: 1.05em;">
        The generated <strong>6D (RGB + XYZ) videos</strong> can be lifted into <strong>dynamic point clouds</strong>.
      </p>
    </font>
    <video width="100%" controls>
      <source src="./assets/point_cloud.mp4" type="video/mp4">
    </video>
    <font size="-0.">
      <p style="text-align: justify; font-size: 1.05em;">
        These dynamic point clouds can be visualized <strong>interactively</strong>.
      </p>
    </font>
    <video width="100%" controls>
      <source src="./assets/viewer.mp4" type="video/mp4">
    </video>
  </div>


  <div class="content">
    <h2 style="text-align:center;">Results - Novel-view Video</h2>
    <font size="-0.">
      <p style="text-align: justify; font-size: 1.05em;">
        The generated <strong>dynamic point clouds</strong> empower downstream applications such as <strong>novel-view video synthesis</strong>.
      </p>
    </font>
    <video width="100%" controls>
      <source src="./assets/novel_view_video.mp4" type="video/mp4">
    </video>
  </div>


  <div class="content">
    <h2>BibTex</h2>
    <font size="-0.">
  </font>
  </div>
  
  <div class="content" id="acknowledgements">
    <p><strong>Acknowledgements</strong>:
      Thanks to <a href="https://monst3r-project.github.io/">MonST3R</a> for the website template and interactive 4D visualization.
      <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). -->
    </p>
  </div>
</body>

</html>
